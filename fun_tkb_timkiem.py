# fun_tkb_timkiem.py
import streamlit as st
import pandas as pd
import re
import gspread
from google.oauth2.service_account import Credentials
from urllib.parse import quote_plus

# --- C√ÅC H√ÄM K·∫æT N·ªêI V√Ä ƒê·ªåC GOOGLE SHEETS (KH√îNG THAY ƒê·ªîI) ---

@st.cache_resource
def connect_to_gsheet():
    try:
        creds_dict = st.secrets["gcp_service_account"]
        creds = Credentials.from_service_account_info(
            creds_dict,
            scopes=["https://www.googleapis.com/auth/spreadsheets"],
        )
        client = gspread.authorize(creds)
        return client
    except Exception as e:
        st.error(f"L·ªói k·∫øt n·ªëi Google Sheets: {e}")
        return None

@st.cache_data(ttl=60)
def get_all_data_sheets(_client, spreadsheet_id):
    if not _client: return []
    try:
        spreadsheet = _client.open_by_key(spreadsheet_id)
        return [s.title for s in spreadsheet.worksheets() if s.title.startswith("DATA_")]
    except Exception as e:
        st.error(f"L·ªói khi l·∫•y danh s√°ch sheet: {e}"); return []

@st.cache_data(ttl=60)
def load_data_from_gsheet(_client, spreadsheet_id, sheet_name):
    if not _client or not sheet_name: return pd.DataFrame()
    try:
        spreadsheet = _client.open_by_key(spreadsheet_id)
        worksheet = spreadsheet.worksheet(sheet_name)
        df = pd.DataFrame(worksheet.get_all_records())
        for col in ['Th·ª©', 'Ti·∫øt']:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        return df
    except Exception as e:
        st.error(f"L·ªói khi t·∫£i d·ªØ li·ªáu t·ª´ sheet '{sheet_name}': {e}"); return pd.DataFrame()

@st.cache_data(ttl=300)
def load_all_data_and_get_dates(_client, spreadsheet_id):
    if not _client:
        return pd.DataFrame(), []
    try:
        sheet_list = get_all_data_sheets(_client, spreadsheet_id)
        if not sheet_list:
            return pd.DataFrame(), []
        all_dfs = []
        for sheet_name in sheet_list:
            df = load_data_from_gsheet(_client, spreadsheet_id, sheet_name)
            if not df.empty:
                all_dfs.append(df)
        if not all_dfs:
            return pd.DataFrame(), []
        combined_df = pd.concat(all_dfs, ignore_index=True)
        if 'Ng√†y √°p d·ª•ng' in combined_df.columns:
            valid_dates_series = pd.to_datetime(combined_df['Ng√†y √°p d·ª•ng'], dayfirst=True, errors='coerce')
            date_list = sorted(valid_dates_series.dropna().dt.strftime('%d/%m/%Y').unique())
            combined_df['Ng√†y √°p d·ª•ng'] = valid_dates_series.dt.strftime('%d/%m/%Y')
        else:
            date_list = []
        return combined_df, date_list
    except Exception as e:
        st.error(f"L·ªói khi t·∫£i v√† h·ª£p nh·∫•t d·ªØ li·ªáu: {e}")
        return pd.DataFrame(), []

# --- H√ÄM HI·ªÇN TH·ªä CHI TI·∫æT L·ªäCH H·ªåC (ƒê√É C·∫¨P NH·∫¨T LINK ƒê·ªÇ KH√îNG B·ªä LOGOUT) ---
def render_schedule_details(schedule_df, mode='class'):
    """H√†m chung ƒë·ªÉ hi·ªÉn th·ªã chi ti·∫øt l·ªãch h·ªçc ho·∫∑c l·ªãch d·∫°y."""
    green_color = "#00FF00"
    number_to_day_map = {
        2: '2Ô∏è‚É£ TH·ª® HAI', 3: '3Ô∏è‚É£ TH·ª® BA', 4: '4Ô∏è‚É£ TH·ª® T∆Ø',
        5: '5Ô∏è‚É£ TH·ª® NƒÇM', 6: '6Ô∏è‚É£ TH·ª® S√ÅU', 7: '7Ô∏è‚É£ TH·ª® B·∫¢Y'
    }
    schedule_df['Th·ª© ƒê·∫ßy ƒê·ªß'] = schedule_df['Th·ª©'].map(number_to_day_map)

    day_order = list(number_to_day_map.values()); session_order = ['S√°ng', 'Chi·ªÅu']
    schedule_df['Th·ª© ƒê·∫ßy ƒê·ªß'] = pd.Categorical(schedule_df['Th·ª© ƒê·∫ßy ƒê·ªß'], categories=day_order, ordered=True)
    schedule_df['Bu·ªïi'] = pd.Categorical(schedule_df['Bu·ªïi'], categories=session_order, ordered=True)
    schedule_sorted = schedule_df.sort_values(by=['Th·ª© ƒê·∫ßy ƒê·ªß', 'Bu·ªïi', 'Ti·∫øt'])

    for day, day_group in schedule_sorted.groupby('Th·ª© ƒê·∫ßy ƒê·ªß', observed=False):
        if day_group['M√¥n h·ªçc'].dropna().empty:
            continue

        st.markdown(f"##### <b>{day}</b> <span style='color:white; font-weight: normal; margin-left: 10px;'>--------------------</span>", unsafe_allow_html=True)

        can_consolidate = False
        if set(day_group['Bu·ªïi'].unique()) == {'S√°ng', 'Chi·ªÅu'}:
            sang_subjects = day_group[day_group['Bu·ªïi'] == 'S√°ng'][['M√¥n h·ªçc', 'Gi√°o vi√™n BM', 'Ph√≤ng h·ªçc', 'L·ªõp']].drop_duplicates()
            chieu_subjects = day_group[day_group['Bu·ªïi'] == 'Chi·ªÅu'][['M√¥n h·ªçc', 'Gi√°o vi√™n BM', 'Ph√≤ng h·ªçc', 'L·ªõp']].drop_duplicates()
            if len(sang_subjects) == 1 and sang_subjects.equals(chieu_subjects): can_consolidate = True

        if can_consolidate:
            st.markdown(f'<p style="color:#17a2b8; font-weight:bold;">C·∫¢ NG√ÄY</p>', unsafe_allow_html=True)
            subject_info = sang_subjects.iloc[0]
            tiet_str = ", ".join(sorted(day_group['Ti·∫øt'].astype(str).tolist(), key=int))

            details = []
            
            mon_hoc_text = subject_info['M√¥n h·ªçc']
            mon_hoc_encoded = quote_plus(mon_hoc_text)
            # Link ƒë√∫ng: tr·ªè t·ªõi file 'pages/2_thongtin_monhoc.py'
            mon_hoc_link = f"<a href='2_thongtin_monhoc?monhoc={mon_hoc_encoded}' target='_self' style='color:{green_color}; text-decoration: none;'>{mon_hoc_text}</a>"
            details.append(f"<b>üìñ M√¥n:</b> {mon_hoc_link}")

            details.append(f"<b>‚è∞ Ti·∫øt:</b> <span style='color:{green_color};'>{tiet_str}</span>")

            if mode == 'class':
                if subject_info['Gi√°o vi√™n BM']: details.append(f"<b>üßë‚Äçüíº GV:</b> <span style='color:{green_color};'>{subject_info['Gi√°o vi√™n BM']}</span>")
            else:
                if subject_info['L·ªõp']: details.append(f"<b>üìù L·ªõp:</b> <span style='color:{green_color};'>{subject_info['L·ªõp']}</span>")

            if subject_info['Ph√≤ng h·ªçc']:
                phong_hoc_text = subject_info['Ph√≤ng h·ªçc']
                phong_hoc_encoded = quote_plus(phong_hoc_text)
                # <<< S·ª¨A L·ªñI T·∫†I ƒê√ÇY >>>
                # Link ƒë√∫ng: tr·ªè t·ªõi file 'pages/2_sodo_phonghoc.py'
                phong_hoc_link = f"<a href='2_sodo_phonghoc?phong={phong_hoc_encoded}' target='_self' style='color:{green_color}; text-decoration: none;'>{phong_hoc_text}</a>"
                details.append(f"<b>üè§ Ph√≤ng:</b> {phong_hoc_link}")

            details_html = "<br>".join(f"&nbsp;&nbsp;{item}" for item in details)
            st.markdown(f"<div>{details_html}</div>", unsafe_allow_html=True)

        else:
            for session, session_group in day_group.groupby('Bu·ªïi', observed=False):
                if session_group['M√¥n h·ªçc'].dropna().empty: continue

                color = "#28a745" if session == "S√°ng" else "#dc3545"
                st.markdown(f'<p style="color:{color}; font-weight:bold;">{session.upper()}</p>', unsafe_allow_html=True)

                subjects_in_session = {}
                for _, row in session_group.iterrows():
                    if pd.notna(row['M√¥n h·ªçc']) and row['M√¥n h·ªçc'].strip():
                        key = (row['M√¥n h·ªçc'], row['Gi√°o vi√™n BM'], row['Ph√≤ng h·ªçc'], row['Ghi ch√∫'], row.get('Ng√†y √°p d·ª•ng', ''), row.get('L·ªõp', ''))
                        if key not in subjects_in_session: subjects_in_session[key] = []
                        subjects_in_session[key].append(str(row['Ti·∫øt']))

                if not subjects_in_session:
                    st.markdown("&nbsp;&nbsp;‚ú®Ngh·ªâ")
                else:
                    for (subject, gv, phong, ghi_chu, ngay_ap_dung, lop), tiet_list in subjects_in_session.items():
                        tiet_str = ", ".join(sorted(tiet_list, key=int))

                        details = []
                        
                        mon_hoc_encoded = quote_plus(subject)
                        # Link ƒë√∫ng: tr·ªè t·ªõi file 'pages/2_thongtin_monhoc.py'
                        mon_hoc_link = f"<a href='2_thongtin_monhoc?monhoc={mon_hoc_encoded}' target='_self' style='color:{green_color}; text-decoration: none;'>{subject}</a>"
                        details.append(f"<b>üìñ M√¥n:</b> {mon_hoc_link}")
                        
                        details.append(f"<b>‚è∞ Ti·∫øt:</b> <span style='color:{green_color};'>{tiet_str}</span>")

                        if mode == 'class':
                            if gv: details.append(f"<b>üßë‚Äçüíº GV:</b> <span style='color:{green_color};'>{gv}</span>")
                        else:
                            if lop: details.append(f"<b>üìù L·ªõp:</b> <span style='color:{green_color};'>{lop}</span>")

                        if phong:
                            phong_hoc_encoded = quote_plus(phong)
                            # <<< S·ª¨A L·ªñI T·∫†I ƒê√ÇY >>>
                            # Link ƒë√∫ng: tr·ªè t·ªõi file 'pages/2_sodo_phonghoc.py'
                            phong_hoc_link = f"<a href='2_sodo_phonghoc?phong={phong_hoc_encoded}' target='_self' style='color:{green_color}; text-decoration: none;'>{phong}</a>"
                            details.append(f"<b>üè§ Ph√≤ng:</b> {phong_hoc_link}")

                        ghi_chu_part = ""
                        if ghi_chu and "h·ªçc t·ª´" in ghi_chu.lower():
                            date_match = re.search(r'(\d+/\d+)', ghi_chu)
                            if date_match:
                                ghi_chu_part = f"<b>üîú B·∫Øt ƒë·∫ßu h·ªçc t·ª´:</b> <span style='color:{green_color};'>\"{date_match.group(1)}\"</span>"
                        elif ngay_ap_dung and str(ngay_ap_dung).strip():
                            ghi_chu_part = f"<b>üîú B·∫Øt ƒë·∫ßu h·ªçc t·ª´:</b> <span style='color:{green_color};'>\"{ngay_ap_dung}\"</span>"

                        if ghi_chu_part:
                            details.append(ghi_chu_part)

                        details_html = "<br>".join(f"&nbsp;&nbsp;{item}" for item in details)
                        st.markdown(f"<div>{details_html}</div><br>", unsafe_allow_html=True)
